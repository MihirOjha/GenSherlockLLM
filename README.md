# ğŸ•µï¸â€â™‚ï¸ Sherlock HoLLMs â€” A Fine-Tuned GPT-2 Model on the Sherlock Holmes Canon

Sherlock HoLLMs is a lightweight language model fine-tuned on **all Sherlock Holmes novels and short stories by Sir Arthur Conan Doyle**.

The goal is to generate _Holmes-style prose_ â€” short deductions, philosophical reflections, and Victorian-era storytelling with a Sherlockian tone.

This project demonstrates:

- ğŸ“š Dataset preparation from public-domain literature
- ğŸ§  Fine-tuning GPT-2 using LoRA adapters (parameter-efficient training)
- âš™ï¸ A reproducible training pipeline
- ğŸ“ An inference script for generating new Holmes-like text
- ğŸš€ A complete end-to-end ML project suitable for learning or portfolio work

---

## ğŸ” Desired Example Outputs

**Prompt:** _"Holmes once said to Watson about courage and intellect:"_

**Generated:**

> "Holmes once said to Watson about courage and intellect: 'You perceive, my dear fellow, that courage without clarity is merely noise, and intellect without conviction is a lantern without a flame.'"

---

**Prompt:** _"What advice would Sherlock Holmes give a young detective?"_

**Generated:**

> "Apply your reason before your reactions, and cultivate the silence in which truth is most inclined to speak."

Now I wasn't able to acheive this functioanlity because of the model I used and the small dataset. Maybe I could have fine tuned it further but I am still researching on the different ways. If you have any ideas, please feel free to share. I would love for this project to actually mimic Sherlock

---

## ğŸ“¦ Project Structure

sherlock-wisdom/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # raw downloaded text files
â”‚ â”œâ”€â”€ cleaned/ # processed, chunked data
â”‚ â””â”€â”€ manifests/ # metadata, logs
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_prep/
â”‚ â”‚ â””â”€â”€ preprocess.py
â”‚ â”œâ”€â”€ training/
â”‚ â”‚ â””â”€â”€ train_lora.py
â”‚ â”œâ”€â”€ eval/
â”‚ â”‚ â””â”€â”€ overlap_check.py
â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â””â”€â”€ app.py
â”‚ â”œâ”€â”€ scheduler/
â”‚ â”‚ â””â”€â”€ poster.py
â”‚ â””â”€â”€ utils/
â”‚ â””â”€â”€ helpers.py
â”œâ”€â”€ experiments/ # where training runs/logs are stored
â”œâ”€â”€ venv/ # your virtual environment
â”œâ”€â”€ requirements.txt # dependencies
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

---

## ğŸ§  Model Architecture

This project fine-tunes:

- **Base Model:** GPT-2 (small)
- **Training Method:** LoRA (Low-Rank Adaptation)
- **Framework:** Hugging Face Transformers
- **Epochs:** 3
- **Sequence Length:** 512
- **Precision:** FP16 (if GPU supports it)

LoRA allows the model to learn stylistic patterns _without_ updating all GPT-2 parameters â€” making training fast, lightweight, and accessible on consumer GPUs.

---

## ğŸ“š Dataset

All Sherlock Holmes works included here are in the **public domain**.  
Texts were:

- cleaned
- split into paragraphs
- tokenized
- formatted as plain text for causal LM training

No validation split was used (small corpus + educational project).

---

## âš™ï¸ Installation

```bash
git clone https://github.com/<your-username>/SherlockWisdom.git
cd SherlockWisdom
python -m venv venv
source venv/bin/activate   # or Scripts\activate on Windows
pip install -r requirements.txt
```

## Training

To fine-tune the model from scratch (optional):

python src/train/train_lora.py

This script will:

- Load GPT-2
- Apply LoRA adapters
- Tokenize the Sherlock dataset
- Train for 3 epochs
- Save the LoRA adapter in experiments/sherlock_lora/

## Inference

Generate Holmes-style text with:

python src/eval/infer.py --prompt "Your prompt here"

Example:

python src/eval/infer.py --prompt "Watson was shocked when Sherlock"

## Output

ğŸ” Loading fine-tuned model...

ğŸ§  Sherlock Wisdom:

Watson was shocked when Sherlock Holmes arrived at the door. He had come upon us for some other reason, and could not have expected to find any sign of this man who he knew so well as I did!â€ â€œI am very sorry that you are now in such a bad condition; but it is possible we may be safe here." Watson looked away from him with an expression which seemed rather like resignation: "You will think better if me myself should ask your opinion about these matters before my departure arrives later today--if only after one hour or two!" Then his eyes were full again on our conversation-tableâ€”an extraordinary sight among them all alike except Mr., whom they both admired admirably since their first meeting together twenty years ago between Brother John Hawkins and Drs.-John Wardleton (who would soon become Mrs.â€”Papa) Hallenbeck's great companion until she died last week.]

## ğŸš€ Deployment

This is originally what I had in mind before starting the porject but this is so basic that I do not know what more can I do with this.

- A Twitter/X bot posting â€œHolmes Wisdomâ€ daily
- A Gradio web UI for interactive generation
- A VS Code extension that rewrites text in Sherlockâ€™s voice
- A small API endpoint (FastAPI) for programmatic use

## Future Improvements

- Add a validation split and perplexity evaluation
- Fine-tune on a larger base model (GPT-Neo or GPT-J)
- Use RLHF or reward models to improve quote-style responses
- Add prompt templates for cleaner outputs
- Train on additional Victorian literature for richer consistency

## ğŸ“œ License

- All Sherlock Holmes texts used here are in the public domain.
- Model code is MIT-licensed.
- Model outputs may resemble Conan Doyleâ€™s style but are not copied text.

## ğŸ™Œ Acknowledgments

- Sir Arthur Conan Doyle â€” for the timeless detective himself
- Hugging Face â€” for Transformers, Datasets, and PEFT
- The open-source ML community

If you stumbled upon this project and liked the idea and want to collab, please reach out. I would love feedback and learn so much more.
